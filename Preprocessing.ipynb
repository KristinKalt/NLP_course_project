{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd3f97d0a08476abd5f0decee5759d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_language_data(df):\n",
    "    def get_lang_df(df, language):\n",
    "        return df[df['language'] == language]\n",
    "    return get_lang_df(df_train, 'english').copy(), get_lang_df(df_train, 'finnish').copy(), get_lang_df(df_train, 'japanese').copy()\n",
    "\n",
    "df_train_EN, df_train_FI, df_train_JAP = split_language_data(df_train)\n",
    "df_val_EN, df_val_FI, df_val_JAP = split_language_data(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8786f5",
   "metadata": {},
   "source": [
    "###  (a) Tokenize question and document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    def make_col_answer(text):\n",
    "        return text['answer_text'][0]\n",
    "\n",
    "    def make_col_answer_start(text):\n",
    "        return text['answer_start'][0]\n",
    "\n",
    "    df['answer_text'] = df['annotations'].apply(make_col_answer)\n",
    "    df['answer_start'] = df['annotations'].apply(make_col_answer_start)\n",
    "    df['answerable'] = df['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize(df, col: str, language):\n",
    "    df[col+'_tokens'] = df[col].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(remove_stopwords, language=language)\n",
    "\n",
    "def helper_func_JAP(text):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(text)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df, col):\n",
    "    df[col+'_tokens'] = df[col].apply(helper_func_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize EN\n",
    "prepare_df(df_train_EN)\n",
    "tokenize(df_train_EN, 'question_text', 'english')\n",
    "tokenize(df_train_EN, 'document_plaintext', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize FI\n",
    "prepare_df(df_train_FI)\n",
    "tokenize(df_train_FI, 'question_text', 'finnish')\n",
    "tokenize(df_train_FI, 'document_plaintext', 'finnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize JAP\n",
    "prepare_df(df_train_JAP)\n",
    "tokenize_JAP(df_train_JAP, 'question_text')\n",
    "tokenize_JAP(df_train_JAP, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2b9ac",
   "metadata": {},
   "source": [
    "### (b) Compute most common first and last tokens in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['question_text_tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['question_text_tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "76ddcb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "born           342\n",
       "founded        204\n",
       "die            122\n",
       "have           104\n",
       "formed         100\n",
       "established     96\n",
       "air             82\n",
       "released        80\n",
       "live            76\n",
       "introduced      72\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "\n",
    "def make_bow_get_vocab_size(df):  \n",
    "\n",
    "    def get_question_vocab(df):\n",
    "        token_list_temp = df.question_text_tokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = get_question_vocab(df)\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_question'] = df['question_text_tokens_cleaned'].apply(transform_bow)\n",
    "    return len(vocab)\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_text_tokens_cleaned']) & set(row['document_plaintext_tokens_cleaned'])))\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden=8):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771661",
   "metadata": {},
   "source": [
    "### Testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2045df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixe X test:  torch.Size([10, 7])\n",
      "epoch:  0  loss:  0.6307103633880615\n"
     ]
    }
   ],
   "source": [
    "#Test data\n",
    "x_test = torch.randn(10, 7)\n",
    "y_test = torch.tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "print(\"Sixe X test: \", x_test.size())\n",
    "\n",
    "test_model = BoWClassifier(num_labels=2, vocab_size=7, num_hidden=8)\n",
    "train_loop(test_model, loss_function, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad125947",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "90263734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    temp_list = list(df['bow_question'].values)\n",
    "    temp_list_ii = [x[0] for x in temp_list]\n",
    "    y_list = list(df['answerable'].values)\n",
    "\n",
    "    X = torch.FloatTensor(temp_list_ii)\n",
    "    y = torch.tensor(y_list)\n",
    "\n",
    "    return train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51514bc",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "019d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, X, y):\n",
    "\n",
    "    def train_loop(model, X, y):   \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        #compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_function(y_pred,y)\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad() #cleans the gradients\n",
    "        loss.backward() #computes the gradients\n",
    "        optimizer.step() #update the parameters\n",
    "            \n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        train_loop(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d744c",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3957b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        y_pred_test = model(X)\n",
    "        loss_test = loss_function(y_pred_test, y)\n",
    "        total_loss += float(loss_test)\n",
    "    print(\"Total loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755541b3",
   "metadata": {},
   "source": [
    "### Calling the methods for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476c2f4",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5867c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch:  0  loss:  0.7165907025337219\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch:  1  loss:  0.7141585350036621\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch:  2  loss:  0.7119473814964294\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "epoch:  3  loss:  0.7099424600601196\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "epoch:  4  loss:  0.7081308960914612\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "epoch:  5  loss:  0.7065021395683289\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "epoch:  6  loss:  0.7050399780273438\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "epoch:  7  loss:  0.7037283778190613\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "epoch:  8  loss:  0.7025529742240906\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "epoch:  9  loss:  0.7015030384063721\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "epoch:  10  loss:  0.7005650401115417\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "epoch:  11  loss:  0.6997271776199341\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "epoch:  12  loss:  0.698979914188385\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "epoch:  13  loss:  0.6983144879341125\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "epoch:  14  loss:  0.6977207064628601\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "epoch:  15  loss:  0.697191596031189\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "epoch:  16  loss:  0.6967208981513977\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "epoch:  17  loss:  0.6963014602661133\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "epoch:  18  loss:  0.6959288120269775\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "epoch:  19  loss:  0.6955991387367249\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "epoch:  20  loss:  0.695306658744812\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "epoch:  21  loss:  0.6950479745864868\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "epoch:  22  loss:  0.6948186159133911\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "epoch:  23  loss:  0.6946159601211548\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "epoch:  24  loss:  0.6944372653961182\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "epoch:  25  loss:  0.6942796111106873\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "epoch:  26  loss:  0.6941404938697815\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "epoch:  27  loss:  0.6940178871154785\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "epoch:  28  loss:  0.6939097046852112\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "epoch:  29  loss:  0.6938139796257019\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "epoch:  30  loss:  0.6937292814254761\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "epoch:  31  loss:  0.6936542987823486\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "epoch:  32  loss:  0.6935879588127136\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "epoch:  33  loss:  0.693529486656189\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "epoch:  34  loss:  0.693477988243103\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "epoch:  35  loss:  0.6934323906898499\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "epoch:  36  loss:  0.6933920979499817\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "epoch:  37  loss:  0.6933563947677612\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "epoch:  38  loss:  0.6933246850967407\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "epoch:  39  loss:  0.6932969093322754\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "epoch:  40  loss:  0.6932722330093384\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "epoch:  41  loss:  0.6932503581047058\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "epoch:  42  loss:  0.6932309865951538\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "epoch:  43  loss:  0.6932138800621033\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "epoch:  44  loss:  0.6931986212730408\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "epoch:  45  loss:  0.6931850910186768\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "epoch:  46  loss:  0.6931728720664978\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "epoch:  47  loss:  0.6931618452072144\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "epoch:  48  loss:  0.6931520104408264\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "epoch:  49  loss:  0.6931430101394653\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "epoch:  50  loss:  0.6931350231170654\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "epoch:  51  loss:  0.6931276917457581\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "epoch:  52  loss:  0.693121075630188\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "epoch:  53  loss:  0.6931150555610657\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "epoch:  54  loss:  0.6931095123291016\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "epoch:  55  loss:  0.6931044459342957\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "epoch:  56  loss:  0.6930997371673584\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "epoch:  57  loss:  0.6930953860282898\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "epoch:  58  loss:  0.6930913329124451\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "epoch:  59  loss:  0.6930875182151794\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "epoch:  60  loss:  0.6930841207504272\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "epoch:  61  loss:  0.693080723285675\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "epoch:  62  loss:  0.6930775046348572\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "epoch:  63  loss:  0.6930745244026184\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "epoch:  64  loss:  0.6930716037750244\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "epoch:  65  loss:  0.6930689215660095\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "epoch:  66  loss:  0.6930661797523499\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "epoch:  67  loss:  0.6930637359619141\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "epoch:  68  loss:  0.6930612325668335\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "epoch:  69  loss:  0.6930588483810425\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "epoch:  70  loss:  0.6930564641952515\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "epoch:  71  loss:  0.6930541396141052\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "epoch:  72  loss:  0.6930519342422485\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "epoch:  73  loss:  0.6930496692657471\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "epoch:  74  loss:  0.6930476427078247\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "epoch:  75  loss:  0.6930454969406128\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "epoch:  76  loss:  0.6930433511734009\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "epoch:  77  loss:  0.693041205406189\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "epoch:  78  loss:  0.6930391192436218\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "epoch:  79  loss:  0.6930371522903442\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "epoch:  80  loss:  0.6930350661277771\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "epoch:  81  loss:  0.6930330991744995\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "epoch:  82  loss:  0.6930310726165771\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "epoch:  83  loss:  0.6930291056632996\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "epoch:  84  loss:  0.6930271983146667\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "epoch:  85  loss:  0.6930252313613892\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "epoch:  86  loss:  0.6930232644081116\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "epoch:  87  loss:  0.693021297454834\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "epoch:  88  loss:  0.6930193901062012\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "epoch:  89  loss:  0.6930174827575684\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "epoch:  90  loss:  0.6930155158042908\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "epoch:  91  loss:  0.6930136680603027\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "epoch:  92  loss:  0.6930117011070251\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "epoch:  93  loss:  0.6930097937583923\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "epoch:  94  loss:  0.6930078268051147\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "epoch:  95  loss:  0.6930059790611267\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "epoch:  96  loss:  0.6930040717124939\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "epoch:  97  loss:  0.6930020451545715\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "epoch:  98  loss:  0.6930001974105835\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "epoch:  99  loss:  0.6929982900619507\n",
      "Total loss:  0.6939598917961121\n"
     ]
    }
   ],
   "source": [
    "vocab_size = make_bow_get_vocab_size(df_train_EN)\n",
    "X_train_EN, X_test_EN, y_train_EN, y_test_EN = prep_data(df_train_EN)\n",
    "model_EN = BoWClassifier(num_labels=2, vocab_size=vocab_size, num_hidden=7)\n",
    "train_model(model_EN, X_train_EN, y_train_EN)\n",
    "test_model(model_EN, X_test_EN, y_test_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df4607",
   "metadata": {},
   "source": [
    "### FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "2472ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10960x9617 and 50060x7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [300], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_FI, X_test_FI, y_train_FI, y_test_FI \u001b[38;5;241m=\u001b[39m prep_data(df_train_FI)\n\u001b[1;32m      3\u001b[0m model_FI \u001b[38;5;241m=\u001b[39m BoWClassifier(num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, num_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_FI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_FI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_FI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_model(model_FI, X_test_FI, y_test_FI)\n",
      "Cell \u001b[0;32mIn [289], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [289], line 7\u001b[0m, in \u001b[0;36mtrain_model.<locals>.train_loop\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#compute prediction and loss\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred,y)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [287], line 11\u001b[0m, in \u001b[0;36mBoWClassifier.forward\u001b[0;34m(self, bow_vector)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, bow_vector):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_vector\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=113'>114</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10960x9617 and 50060x7)"
     ]
    }
   ],
   "source": [
    "vocab_size = make_bow_get_vocab_size(df_train_FI)\n",
    "X_train_FI, X_test_FI, y_train_FI, y_test_FI = prep_data(df_train_FI)\n",
    "model_FI = BoWClassifier(num_labels=2, vocab_size=vocab_size, num_hidden=7)\n",
    "train_model(model_FI, X_train_FI, y_train_FI)\n",
    "test_model(model_FI, X_test_FI, y_test_FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed644fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
