{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de9a868bf7d440c9642a840ed28e9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_language_data(df):\n",
    "    def get_lang_df(df, language):\n",
    "        return df[df['language'] == language]\n",
    "    return get_lang_df(df_train, 'english').copy(), get_lang_df(df_train, 'finnish').copy(), get_lang_df(df_train, 'japanese').copy()\n",
    "\n",
    "df_train_EN, df_train_FI, df_train_JAP = split_language_data(df_train)\n",
    "df_val_EN, df_val_FI, df_val_JAP = split_language_data(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8786f5",
   "metadata": {},
   "source": [
    "###  (a) Tokenize question and document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    def make_col_answer(text):\n",
    "        return text['answer_text'][0]\n",
    "\n",
    "    def make_col_answer_start(text):\n",
    "        return text['answer_start'][0]\n",
    "\n",
    "    df['answer_text'] = df['annotations'].apply(make_col_answer)\n",
    "    df['answer_start'] = df['annotations'].apply(make_col_answer_start)\n",
    "    df['answerable'] = df['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize(df, col: str, language):\n",
    "    df[col+'_tokens'] = df[col].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(remove_stopwords, language=language)\n",
    "\n",
    "def helper_func_JAP(text):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(text)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df, col):\n",
    "    df[col+'_tokens'] = df[col].apply(helper_func_JAP)\n",
    "    df[col+'_tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(helper_func_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4f21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize EN\n",
    "prepare_df(df_train_EN)\n",
    "tokenize(df_train_EN, 'question_text', 'english')\n",
    "tokenize(df_train_EN, 'document_plaintext', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7302c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize FI\n",
    "prepare_df(df_train_FI)\n",
    "tokenize(df_train_FI, 'question_text', 'finnish')\n",
    "tokenize(df_train_FI, 'document_plaintext', 'finnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95efb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize JAP\n",
    "prepare_df(df_train_JAP)\n",
    "tokenize_JAP(df_train_JAP, 'question_text')\n",
    "tokenize_JAP(df_train_JAP, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2b9ac",
   "metadata": {},
   "source": [
    "### (b) Compute most common first and last tokens in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['question_text_tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['question_text_tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "76ddcb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "born           342\n",
       "founded        204\n",
       "die            122\n",
       "have           104\n",
       "formed         100\n",
       "established     96\n",
       "air             82\n",
       "released        80\n",
       "live            76\n",
       "introduced      72\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "522ec7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_text', 'document_title', 'language', 'annotations',\n",
       "       'document_plaintext', 'document_url', 'answer_text', 'answer_start',\n",
       "       'answerable', 'question_text_tokens', 'question_text_tokens_cleaned',\n",
       "       'document_plaintext_tokens', 'document_plaintext_tokens_cleaned',\n",
       "       'bow_question'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_EN.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "\n",
    "def make_bow_get_vocab_size(df):  \n",
    "\n",
    "    def get_question_vocab(df):\n",
    "        token_list_temp = df.joined_tokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "    df['joined_tokens_cleaned'] = df['question_text_tokens_cleaned'] + df['document_plaintext_tokens_cleaned']\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = get_question_vocab(df)\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_joined'] = df['joined_tokens_cleaned'].apply(transform_bow)\n",
    "    return len(df.iloc[0]['bow_joined'][0])\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_text_tokens_cleaned']) & set(row['document_plaintext_tokens_cleaned'])))\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(vocab_size, num_labels)\n",
    "        #self.output = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771661",
   "metadata": {},
   "source": [
    "### Testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2045df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixe X test:  torch.Size([10, 7])\n",
      "epoch:  0  loss:  0.6307103633880615\n"
     ]
    }
   ],
   "source": [
    "#Test data\n",
    "x_test = torch.randn(10, 7)\n",
    "y_test = torch.tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "print(\"Sixe X test: \", x_test.size())\n",
    "\n",
    "test_model = BoWClassifier(num_labels=2, vocab_size=7, num_hidden=8)\n",
    "train_loop(test_model, loss_function, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad125947",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90263734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    temp_list = list(df['bow_joined'].values)\n",
    "    temp_list_ii = [x[0] for x in temp_list]\n",
    "    y_list = list(df['answerable'].values)\n",
    "\n",
    "    X = torch.FloatTensor(temp_list_ii)\n",
    "    y = torch.tensor(y_list)\n",
    "\n",
    "    return train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51514bc",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "019d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, X, y):\n",
    "\n",
    "    def train_loop(model, X, y):   \n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        #compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad() #cleans the gradients\n",
    "        loss.backward() #computes the gradients\n",
    "        optimizer.step() #update the parameters\n",
    "            \n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loop(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d744c",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3957b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        y_pred_test = model(X)\n",
    "        y_pred_test = y_pred_test.detach().cpu().numpy()\n",
    "        y_pred = np.argmax(y_pred_test, axis=1)\n",
    "        y_pred_tensor = torch.FloatTensor(y_pred)\n",
    "\n",
    "        y_true = y.type(torch.FloatTensor)\n",
    "        \n",
    "        loss_test = loss_function(y_pred_tensor, y_true)\n",
    "        total_loss += float(loss_test)\n",
    "    print(\"Total loss on test data: \", total_loss)\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755541b3",
   "metadata": {},
   "source": [
    "### Calling the methods for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476c2f4",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5867c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/z1y7t00n7bd8lm8r0r4sy2b40000gn/T/ipykernel_2385/3282876610.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  X = torch.FloatTensor(temp_list_ii)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6932293772697449\n",
      "epoch:  1  loss:  0.6921654343605042\n",
      "epoch:  2  loss:  0.6913775205612183\n",
      "epoch:  3  loss:  0.6907072067260742\n"
     ]
    }
   ],
   "source": [
    "vocab_size_EN = make_bow_get_vocab_size(df_train_EN)\n",
    "X_train_EN, X_test_EN, y_train_EN, y_test_EN = prep_data(df_train_EN)\n",
    "model_EN = BoWClassifier(num_labels=2, vocab_size=vocab_size_EN, num_hidden=vocab_size_EN)\n",
    "train_model(model_EN, X_train_EN, y_train_EN)\n",
    "test_model(model_EN, X_test_EN, y_test_EN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df4607",
   "metadata": {},
   "source": [
    "### FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2472ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6931490302085876\n",
      "epoch:  1  loss:  0.6931475400924683\n",
      "epoch:  2  loss:  0.6931461691856384\n",
      "epoch:  3  loss:  0.6931449174880981\n",
      "epoch:  4  loss:  0.6931435465812683\n",
      "epoch:  5  loss:  0.693142294883728\n",
      "epoch:  6  loss:  0.6931410431861877\n",
      "epoch:  7  loss:  0.6931398510932922\n",
      "epoch:  8  loss:  0.6931387186050415\n",
      "epoch:  9  loss:  0.693137526512146\n",
      "epoch:  10  loss:  0.6931365132331848\n",
      "epoch:  11  loss:  0.6931353211402893\n",
      "epoch:  12  loss:  0.6931342482566833\n",
      "epoch:  13  loss:  0.6931332349777222\n",
      "epoch:  14  loss:  0.6931321024894714\n",
      "epoch:  15  loss:  0.693131148815155\n",
      "epoch:  16  loss:  0.6931300759315491\n",
      "epoch:  17  loss:  0.6931290626525879\n",
      "epoch:  18  loss:  0.6931279897689819\n",
      "epoch:  19  loss:  0.6931269764900208\n",
      "epoch:  20  loss:  0.6931260824203491\n",
      "epoch:  21  loss:  0.6931250095367432\n",
      "epoch:  22  loss:  0.693123996257782\n",
      "epoch:  23  loss:  0.6931230425834656\n",
      "epoch:  24  loss:  0.6931220293045044\n",
      "epoch:  25  loss:  0.6931210160255432\n",
      "epoch:  26  loss:  0.6931200623512268\n",
      "epoch:  27  loss:  0.6931190490722656\n",
      "epoch:  28  loss:  0.6931179761886597\n",
      "epoch:  29  loss:  0.6931169629096985\n",
      "epoch:  30  loss:  0.6931160092353821\n",
      "epoch:  31  loss:  0.6931149959564209\n",
      "epoch:  32  loss:  0.6931140422821045\n",
      "epoch:  33  loss:  0.6931130290031433\n",
      "epoch:  34  loss:  0.6931120157241821\n",
      "epoch:  35  loss:  0.693111002445221\n",
      "epoch:  36  loss:  0.6931100487709045\n",
      "epoch:  37  loss:  0.6931091547012329\n",
      "epoch:  38  loss:  0.6931081414222717\n",
      "epoch:  39  loss:  0.6931071281433105\n",
      "epoch:  40  loss:  0.6931061744689941\n",
      "epoch:  41  loss:  0.6931052207946777\n",
      "epoch:  42  loss:  0.6931042075157166\n",
      "epoch:  43  loss:  0.6931031942367554\n",
      "epoch:  44  loss:  0.693102240562439\n",
      "epoch:  45  loss:  0.6931012868881226\n",
      "epoch:  46  loss:  0.6931003332138062\n",
      "epoch:  47  loss:  0.693099319934845\n",
      "epoch:  48  loss:  0.6930983662605286\n",
      "epoch:  49  loss:  0.6930973529815674\n",
      "epoch:  50  loss:  0.6930962800979614\n",
      "epoch:  51  loss:  0.6930953860282898\n",
      "epoch:  52  loss:  0.6930943727493286\n",
      "epoch:  53  loss:  0.6930934190750122\n",
      "epoch:  54  loss:  0.6930925250053406\n",
      "epoch:  55  loss:  0.6930915117263794\n",
      "epoch:  56  loss:  0.6930904984474182\n",
      "epoch:  57  loss:  0.6930895447731018\n",
      "epoch:  58  loss:  0.6930885314941406\n",
      "epoch:  59  loss:  0.6930875778198242\n",
      "epoch:  60  loss:  0.6930866241455078\n",
      "epoch:  61  loss:  0.6930856704711914\n",
      "epoch:  62  loss:  0.6930846571922302\n",
      "epoch:  63  loss:  0.6930837035179138\n",
      "epoch:  64  loss:  0.6930826902389526\n",
      "epoch:  65  loss:  0.6930816769599915\n",
      "epoch:  66  loss:  0.693080723285675\n",
      "epoch:  67  loss:  0.6930797696113586\n",
      "epoch:  68  loss:  0.6930788159370422\n",
      "epoch:  69  loss:  0.6930778622627258\n",
      "epoch:  70  loss:  0.6930768489837646\n",
      "epoch:  71  loss:  0.6930758953094482\n",
      "epoch:  72  loss:  0.6930749416351318\n",
      "epoch:  73  loss:  0.6930739283561707\n",
      "epoch:  74  loss:  0.6930729746818542\n",
      "epoch:  75  loss:  0.6930719614028931\n",
      "epoch:  76  loss:  0.6930710673332214\n",
      "epoch:  77  loss:  0.6930700540542603\n",
      "epoch:  78  loss:  0.6930691003799438\n",
      "epoch:  79  loss:  0.6930680274963379\n",
      "epoch:  80  loss:  0.6930670738220215\n",
      "epoch:  81  loss:  0.6930661201477051\n",
      "epoch:  82  loss:  0.6930651068687439\n",
      "epoch:  83  loss:  0.6930642127990723\n",
      "epoch:  84  loss:  0.6930631995201111\n",
      "epoch:  85  loss:  0.6930622458457947\n",
      "epoch:  86  loss:  0.6930612921714783\n",
      "epoch:  87  loss:  0.6930603384971619\n",
      "epoch:  88  loss:  0.6930593848228455\n",
      "epoch:  89  loss:  0.6930583715438843\n",
      "epoch:  90  loss:  0.6930574178695679\n",
      "epoch:  91  loss:  0.6930564045906067\n",
      "epoch:  92  loss:  0.6930555105209351\n",
      "epoch:  93  loss:  0.6930544376373291\n",
      "epoch:  94  loss:  0.6930535435676575\n",
      "epoch:  95  loss:  0.6930525302886963\n",
      "epoch:  96  loss:  0.6930516362190247\n",
      "epoch:  97  loss:  0.6930506825447083\n",
      "epoch:  98  loss:  0.6930496096611023\n",
      "epoch:  99  loss:  0.6930486559867859\n",
      "Total loss on test data:  10701.5517578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.02      0.04      1393\n",
      "         1.0       0.49      0.96      0.64      1348\n",
      "\n",
      "    accuracy                           0.48      2741\n",
      "   macro avg       0.41      0.49      0.34      2741\n",
      "weighted avg       0.41      0.48      0.34      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size_FI = make_bow_get_vocab_size(df_train_FI)\n",
    "X_train_FI, X_test_FI, y_train_FI, y_test_FI = prep_data(df_train_FI)\n",
    "model_FI = BoWClassifier(num_labels=2, vocab_size=vocab_size_FI, num_hidden=vocab_size_FI)\n",
    "train_model(model_FI, X_train_FI, y_train_FI)\n",
    "test_model(model_FI, X_test_FI, y_test_FI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed644fd",
   "metadata": {},
   "source": [
    "### JAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b945827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6932111382484436\n",
      "epoch:  1  loss:  0.6932008266448975\n",
      "epoch:  2  loss:  0.6931919455528259\n",
      "epoch:  3  loss:  0.693183958530426\n",
      "epoch:  4  loss:  0.6931769251823425\n",
      "epoch:  5  loss:  0.6931706666946411\n",
      "epoch:  6  loss:  0.6931650638580322\n",
      "epoch:  7  loss:  0.6931599974632263\n",
      "epoch:  8  loss:  0.6931554675102234\n",
      "epoch:  9  loss:  0.6931514143943787\n",
      "epoch:  10  loss:  0.6931475400924683\n",
      "epoch:  11  loss:  0.6931442022323608\n",
      "epoch:  12  loss:  0.6931410431861877\n",
      "epoch:  13  loss:  0.693138062953949\n",
      "epoch:  14  loss:  0.6931353211402893\n",
      "epoch:  15  loss:  0.6931326985359192\n",
      "epoch:  16  loss:  0.693130373954773\n",
      "epoch:  17  loss:  0.6931279301643372\n",
      "epoch:  18  loss:  0.6931257247924805\n",
      "epoch:  19  loss:  0.6931235790252686\n",
      "epoch:  20  loss:  0.6931215524673462\n",
      "epoch:  21  loss:  0.6931195855140686\n",
      "epoch:  22  loss:  0.6931174993515015\n",
      "epoch:  23  loss:  0.6931157112121582\n",
      "epoch:  24  loss:  0.6931138634681702\n",
      "epoch:  25  loss:  0.6931120157241821\n",
      "epoch:  26  loss:  0.6931103467941284\n",
      "epoch:  27  loss:  0.6931085586547852\n",
      "epoch:  28  loss:  0.6931068301200867\n",
      "epoch:  29  loss:  0.693105161190033\n",
      "epoch:  30  loss:  0.6931034326553345\n",
      "epoch:  31  loss:  0.6931017637252808\n",
      "epoch:  32  loss:  0.693100094795227\n",
      "epoch:  33  loss:  0.6930984258651733\n",
      "epoch:  34  loss:  0.6930968165397644\n",
      "epoch:  35  loss:  0.6930950880050659\n",
      "epoch:  36  loss:  0.693093478679657\n",
      "epoch:  37  loss:  0.6930918097496033\n",
      "epoch:  38  loss:  0.6930902004241943\n",
      "epoch:  39  loss:  0.6930885910987854\n",
      "epoch:  40  loss:  0.6930869221687317\n",
      "epoch:  41  loss:  0.6930853128433228\n",
      "epoch:  42  loss:  0.6930837631225586\n",
      "epoch:  43  loss:  0.6930822134017944\n",
      "epoch:  44  loss:  0.6930805444717407\n",
      "epoch:  45  loss:  0.6930789351463318\n",
      "epoch:  46  loss:  0.6930773258209229\n",
      "epoch:  47  loss:  0.6930756568908691\n",
      "epoch:  48  loss:  0.6930741667747498\n",
      "epoch:  49  loss:  0.6930725574493408\n",
      "epoch:  50  loss:  0.6930709481239319\n",
      "epoch:  51  loss:  0.6930693984031677\n",
      "epoch:  52  loss:  0.6930677890777588\n",
      "epoch:  53  loss:  0.6930662989616394\n",
      "epoch:  54  loss:  0.6930646300315857\n",
      "epoch:  55  loss:  0.6930630803108215\n",
      "epoch:  56  loss:  0.6930614113807678\n",
      "epoch:  57  loss:  0.6930598616600037\n",
      "epoch:  58  loss:  0.6930583119392395\n",
      "epoch:  59  loss:  0.6930567622184753\n",
      "epoch:  60  loss:  0.6930551528930664\n",
      "epoch:  61  loss:  0.6930535435676575\n",
      "epoch:  62  loss:  0.6930520534515381\n",
      "epoch:  63  loss:  0.6930503845214844\n",
      "epoch:  64  loss:  0.6930488348007202\n",
      "epoch:  65  loss:  0.6930471658706665\n",
      "epoch:  66  loss:  0.6930456161499023\n",
      "epoch:  67  loss:  0.6930440068244934\n",
      "epoch:  68  loss:  0.693042516708374\n",
      "epoch:  69  loss:  0.6930409073829651\n",
      "epoch:  70  loss:  0.6930392980575562\n",
      "epoch:  71  loss:  0.6930376291275024\n",
      "epoch:  72  loss:  0.6930360794067383\n",
      "epoch:  73  loss:  0.6930345892906189\n",
      "epoch:  74  loss:  0.6930329203605652\n",
      "epoch:  75  loss:  0.693031370639801\n",
      "epoch:  76  loss:  0.6930298209190369\n",
      "epoch:  77  loss:  0.6930281519889832\n",
      "epoch:  78  loss:  0.6930266618728638\n",
      "epoch:  79  loss:  0.6930250525474548\n",
      "epoch:  80  loss:  0.6930234432220459\n",
      "epoch:  81  loss:  0.6930220127105713\n",
      "epoch:  82  loss:  0.6930204629898071\n",
      "epoch:  83  loss:  0.6930188536643982\n",
      "epoch:  84  loss:  0.6930171847343445\n",
      "epoch:  85  loss:  0.6930157542228699\n",
      "epoch:  86  loss:  0.6930140852928162\n",
      "epoch:  87  loss:  0.6930124759674072\n",
      "epoch:  88  loss:  0.6930109858512878\n",
      "epoch:  89  loss:  0.6930093169212341\n",
      "epoch:  90  loss:  0.6930078268051147\n",
      "epoch:  91  loss:  0.6930062770843506\n",
      "epoch:  92  loss:  0.6930046081542969\n",
      "epoch:  93  loss:  0.6930030584335327\n",
      "epoch:  94  loss:  0.6930015683174133\n",
      "epoch:  95  loss:  0.6929998993873596\n",
      "epoch:  96  loss:  0.6929983496665955\n",
      "epoch:  97  loss:  0.6929967999458313\n",
      "epoch:  98  loss:  0.6929951906204224\n",
      "epoch:  99  loss:  0.6929936408996582\n",
      "Total loss on test data:  6779.6630859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.96      0.64       852\n",
      "         1.0       0.40      0.03      0.05       904\n",
      "\n",
      "    accuracy                           0.48      1756\n",
      "   macro avg       0.44      0.49      0.34      1756\n",
      "weighted avg       0.44      0.48      0.34      1756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size_JAP = make_bow_get_vocab_size(df_train_JAP)\n",
    "X_train_JAP, X_test_JAP, y_train_JAP, y_test_JAP = prep_data(df_train_JAP)\n",
    "model_JAP = BoWClassifier(num_labels=2, vocab_size=vocab_size_JAP, num_hidden=vocab_size_JAP)\n",
    "train_model(model_JAP, X_train_JAP, y_train_JAP)\n",
    "test_model(model_JAP, X_test_JAP, y_test_JAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9c75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
