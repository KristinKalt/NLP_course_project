{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7abd7e5dea4b0f99cf482462ea09d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29868"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_df(df, language):\n",
    "    return df[df['language'] == language]\n",
    "\n",
    "df_train_FI = get_lang_df(df_train, 'finnish').copy()\n",
    "df_train_JAP = get_lang_df(df_train, 'japanese').copy()\n",
    "df_train_EN = get_lang_df(df_train, 'english').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize_EN(df, col: str):\n",
    "    df[col+'tokens'] = df[col].apply(word_tokenize)\n",
    "    df[col+'tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'tokens_cleaned'] = df[col+'tokens_cleaned'].apply(word_tokenize)\n",
    "    df[col+'tokens_cleaned'] = df[col+'tokens_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "def tokenize_FI(df):\n",
    "    df['tokens'] = df['question_text'].apply(word_tokenize, language='finnish')\n",
    "\n",
    "def helper_func_JAP(question):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(question)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df):\n",
    "    df['tokens'] = df['question_text'].apply(helper_func_JAP)\n",
    "\n",
    "\n",
    "tokenize_EN(df_train_EN, 'question_text')\n",
    "#tokenize_FI(df_train_FI)\n",
    "#tokenize_JAP(df_train_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3798'>3799</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mlast_token\u001b[38;5;241m.\u001b[39mvalue_counts()[:n]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#For English\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_most_common_first_n_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_EN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m get_most_common_last_n_tokens(df_train_EN, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m, in \u001b[0;36mget_most_common_first_n_tokens\u001b[0;34m(df, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_most_common_first_n_tokens\u001b[39m(df, n):\n\u001b[0;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_token\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mfirst_token\u001b[38;5;241m.\u001b[39mvalue_counts()[:n]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3802'>3803</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3803'>3804</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3804'>3805</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3805'>3806</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3806'>3807</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3803'>3804</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3804'>3805</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3805'>3806</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3806'>3807</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'"
     ]
    }
   ],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]\n",
    "\n",
    "#For English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/z1y7t00n7bd8lm8r0r4sy2b40000gn/T/ipykernel_3860/2224523845.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  res_JAP.to_latex()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llrlr}\\n\\\\toprule\\n{} &        0 &    1 &   2 &     3 \\\\\\\\\\n\\\\midrule\\n0 &       日本 &  354 &   た &  2115 \\\\\\\\\\n1 &        『 &  306 &   か &  1305 \\\\\\\\\\n2 &       世界 &   94 &   何 &  1192 \\\\\\\\\\n3 &      ジョン &   58 &  いつ &   996 \\\\\\\\\\n4 &        第 &   56 &   は &   932 \\\\\\\\\\n5 &  アメリカ合衆国 &   54 &  どこ &   884 \\\\\\\\\\n6 &        「 &   50 &   誰 &   746 \\\\\\\\\\n7 &     アメリカ &   50 &  ある &   174 \\\\\\\\\\n8 &    ウィリアム &   44 &  だれ &    64 \\\\\\\\\\n9 &     ジョージ &   44 &  いる &    42 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)\n",
    "res_JAP.to_latex()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61501f79",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0173d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>question_texttokens</th>\n",
       "      <th>question_texttokens_cleaned</th>\n",
       "      <th>doc_tokens</th>\n",
       "      <th>word_count_doc</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [159], 'answer_text': ['1920s']}</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[quantum, field, theory, developed]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>31</td>\n",
       "      <td>1920s</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [610], 'answer_text': ['Sully...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[first, nobel, prize, winner, literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>188</td>\n",
       "      <td>Sully Prudhomme</td>\n",
       "      <td>610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [129], 'answer_text': ['disco...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[dialectical, method, used]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>113</td>\n",
       "      <td>discourse between two or more people holding d...</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [88], 'answer_text': ['Sejong...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[invented, hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>69</td>\n",
       "      <td>Sejong the Great</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [0], 'answer_text': ['Grassho...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[grasshoppers, eat]</td>\n",
       "      <td>[Grasshoppers, are, plant-eaters, ,, with, a, ...</td>\n",
       "      <td>125</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question_text  \\\n",
       "26            When was quantum field theory developed?   \n",
       "43   Who was the first Nobel prize winner for Liter...   \n",
       "112               When is the dialectical method used?   \n",
       "123                               Who invented Hangul?   \n",
       "125                          What do Grasshoppers eat?   \n",
       "\n",
       "                            document_title language  \\\n",
       "26                    Quantum field theory  english   \n",
       "43   List of Nobel laureates in Literature  english   \n",
       "112                              Dialectic  english   \n",
       "123                       Origin of Hangul  english   \n",
       "125                            Grasshopper  english   \n",
       "\n",
       "                                           annotations  \\\n",
       "26   {'answer_start': [159], 'answer_text': ['1920s']}   \n",
       "43   {'answer_start': [610], 'answer_text': ['Sully...   \n",
       "112  {'answer_start': [129], 'answer_text': ['disco...   \n",
       "123  {'answer_start': [88], 'answer_text': ['Sejong...   \n",
       "125  {'answer_start': [0], 'answer_text': ['Grassho...   \n",
       "\n",
       "                                    document_plaintext  \\\n",
       "26   Quantum field theory naturally began with the ...   \n",
       "43   The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "112  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "123  Hangul was personally created and promulgated ...   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                          document_url  \\\n",
       "26   https://en.wikipedia.org/wiki/Quantum%20field%...   \n",
       "43   https://en.wikipedia.org/wiki/List%20of%20Nobe...   \n",
       "112            https://en.wikipedia.org/wiki/Dialectic   \n",
       "123  https://en.wikipedia.org/wiki/Origin%20of%20Ha...   \n",
       "125          https://en.wikipedia.org/wiki/Grasshopper   \n",
       "\n",
       "                                   question_texttokens  \\\n",
       "26   [When, was, quantum, field, theory, developed, ?]   \n",
       "43   [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "112      [When, is, the, dialectical, method, used, ?]   \n",
       "123                         [Who, invented, Hangul, ?]   \n",
       "125                   [What, do, Grasshoppers, eat, ?]   \n",
       "\n",
       "                   question_texttokens_cleaned  \\\n",
       "26         [quantum, field, theory, developed]   \n",
       "43   [first, nobel, prize, winner, literature]   \n",
       "112                [dialectical, method, used]   \n",
       "123                         [invented, hangul]   \n",
       "125                        [grasshoppers, eat]   \n",
       "\n",
       "                                            doc_tokens  word_count_doc  \\\n",
       "26   [Quantum, field, theory, naturally, began, wit...              31   \n",
       "43   [The, Nobel, Prize, in, Literature, (, Swedish...             188   \n",
       "112  [Dialectic, or, dialectics, (, Greek, :, διαλε...             113   \n",
       "123  [Hangul, was, personally, created, and, promul...              69   \n",
       "125  [Grasshoppers, are, plant-eaters, ,, with, a, ...             125   \n",
       "\n",
       "                                           answer_text  answer_start  \\\n",
       "26                                               1920s           159   \n",
       "43                                     Sully Prudhomme           610   \n",
       "112  discourse between two or more people holding d...           129   \n",
       "123                                   Sejong the Great            88   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...             0   \n",
       "\n",
       "     answerable  \n",
       "26            1  \n",
       "43            1  \n",
       "112           1  \n",
       "123           1  \n",
       "125           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_col_answer(text):\n",
    "    return text['answer_text'][0]\n",
    "\n",
    "def make_col_answer_start(text):\n",
    "    return text['answer_start'][0]\n",
    "\n",
    "df_train_EN['answer_text'] = df_train_EN['annotations'].apply(make_col_answer)\n",
    "df_train_EN['answer_start'] = df_train_EN['annotations'].apply(make_col_answer_start)\n",
    "df_train_EN['answerable'] = df_train_EN['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "df_train_EN.head()\n",
    "\n",
    "tokenize_EN(df_train_EN, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3243f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train_EN[df_train_EN['answerable'] == 0].iloc[82]['document_plaintexttokens_cleaned'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4575"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "def get_question_vocab():\n",
    "        token_list_temp = df_train_EN.question_texttokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "\n",
    "def get_bow(df, vocab):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_question'] = df['question_texttokens_cleaned'].apply(transform_bow)\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_texttokens_cleaned']) & set(row['document_plaintexttokens_cleaned'])))\n",
    "    #df['overlap_doc_question'] = df['question_texttokens_cleaned'].apply(set) & df['document_plaintexttokens_cleaned'].apply(set)\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#count_words_in_doc(df_train_EN)\n",
    "vocab = get_question_vocab()\n",
    "get_bow(df_train_EN, vocab)\n",
    "#get_overlap(df_train_EN)\n",
    "len(df_train_EN.iloc[0]['bow_question'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden=8):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2045df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixe X test:  torch.Size([10, 7])\n",
      "epoch:  0  loss:  0.6307103633880615\n"
     ]
    }
   ],
   "source": [
    "#Test data\n",
    "x_test = torch.randn(10, 7)\n",
    "y_test = torch.tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "print(\"Sixe X test: \", x_test.size())\n",
    "\n",
    "test_model = BoWClassifier(num_labels=2, vocab_size=7, num_hidden=8)\n",
    "train_loop(test_model, loss_function, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "90263734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp_list = list(df_train_EN['bow_question'].values)\n",
    "temp_list_ii = [x[0] for x in temp_list]\n",
    "\n",
    "X = torch.FloatTensor(temp_list_ii)\n",
    "y_list = list(df_train_EN['answerable'].values)\n",
    "#y = torch.tensor([[x] for x in y_list])\n",
    "y = torch.tensor(y_list)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e6a4bddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5056,  0.9153,  0.4793, -0.6444,  0.3628, -0.8074,  2.7315],\n",
       "        [-1.8338,  0.7448,  0.3746,  0.3131, -0.0850,  0.7846, -0.8020],\n",
       "        [ 0.1749, -0.0328, -1.6972, -0.6483,  0.6256, -0.7786,  0.1086],\n",
       "        [ 0.7864,  0.4207,  0.6837, -0.1010, -1.1849, -0.6070,  0.1704],\n",
       "        [-0.7880,  0.2376, -0.9414, -1.3934, -2.3270, -0.7605,  0.4261],\n",
       "        [-0.0236, -1.6823, -0.7846, -1.0659,  0.6222,  0.6719, -1.3679],\n",
       "        [ 1.9006, -1.6853,  0.8929, -0.5236,  0.7401, -0.8913,  0.0544],\n",
       "        [-0.7086,  0.7980,  2.2888, -0.6826,  0.1287, -1.5862, -0.4234],\n",
       "        [ 0.8111,  0.5778, -0.0111, -0.4818, -0.7811, -1.4629,  0.3349],\n",
       "        [ 0.7053, -0.8898,  1.1208, -0.4188, -1.1646,  0.0765, -0.1233]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "019d988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "epoch:  0  loss:  0.7183316349983215\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "epoch:  1  loss:  0.7158913016319275\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "epoch:  2  loss:  0.7136825919151306\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "epoch:  3  loss:  0.7116838693618774\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "epoch:  4  loss:  0.7098755836486816\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "epoch:  5  loss:  0.7082398533821106\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "epoch:  6  loss:  0.706760585308075\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "epoch:  7  loss:  0.7054229974746704\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "epoch:  8  loss:  0.7042140364646912\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "epoch:  9  loss:  0.7031218409538269\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "epoch:  10  loss:  0.7021350860595703\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "epoch:  11  loss:  0.7012439966201782\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "epoch:  12  loss:  0.7004395723342896\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "epoch:  13  loss:  0.6997132897377014\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "epoch:  14  loss:  0.6990580558776855\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "epoch:  15  loss:  0.6984665989875793\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "epoch:  16  loss:  0.6979331970214844\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "epoch:  17  loss:  0.6974521279335022\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "epoch:  18  loss:  0.6970183849334717\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "epoch:  19  loss:  0.6966276168823242\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "epoch:  20  loss:  0.6962754726409912\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "epoch:  21  loss:  0.6959581971168518\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "epoch:  22  loss:  0.6956725120544434\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "epoch:  23  loss:  0.6954153180122375\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "epoch:  24  loss:  0.6951838135719299\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "epoch:  25  loss:  0.6949756741523743\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "epoch:  26  loss:  0.69478839635849\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "epoch:  27  loss:  0.6946200132369995\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "epoch:  28  loss:  0.6944686770439148\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "epoch:  29  loss:  0.6943327188491821\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "epoch:  30  loss:  0.6942105293273926\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "epoch:  31  loss:  0.6941008567810059\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "epoch:  32  loss:  0.6940024495124817\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "epoch:  33  loss:  0.6939140558242798\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "epoch:  34  loss:  0.6938347816467285\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "epoch:  35  loss:  0.6937637329101562\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "epoch:  36  loss:  0.6937000155448914\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "epoch:  37  loss:  0.6936429142951965\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "epoch:  38  loss:  0.6935916543006897\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "epoch:  39  loss:  0.6935458183288574\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "epoch:  40  loss:  0.6935046315193176\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "epoch:  41  loss:  0.6934677362442017\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "epoch:  42  loss:  0.6934347152709961\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "epoch:  43  loss:  0.6934052109718323\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "epoch:  44  loss:  0.693378746509552\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "epoch:  45  loss:  0.6933550238609314\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "epoch:  46  loss:  0.6933338642120361\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "epoch:  47  loss:  0.6933149695396423\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "epoch:  48  loss:  0.6932979226112366\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "epoch:  49  loss:  0.6932827234268188\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "epoch:  50  loss:  0.6932691931724548\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "epoch:  51  loss:  0.6932570338249207\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "epoch:  52  loss:  0.6932461857795715\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "epoch:  53  loss:  0.6932364702224731\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "epoch:  54  loss:  0.6932277679443359\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "epoch:  55  loss:  0.6932199597358704\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "epoch:  56  loss:  0.6932129263877869\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "epoch:  57  loss:  0.693206787109375\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "epoch:  58  loss:  0.6932012438774109\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "epoch:  59  loss:  0.6931962966918945\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "epoch:  60  loss:  0.6931918859481812\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "epoch:  61  loss:  0.693187952041626\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "epoch:  62  loss:  0.6931843757629395\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "epoch:  63  loss:  0.6931811571121216\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "epoch:  64  loss:  0.6931782960891724\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "epoch:  65  loss:  0.6931757926940918\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "epoch:  66  loss:  0.6931735277175903\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "epoch:  67  loss:  0.6931715607643127\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "epoch:  68  loss:  0.6931697726249695\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "epoch:  69  loss:  0.6931681036949158\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "epoch:  70  loss:  0.6931665539741516\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "epoch:  71  loss:  0.6931653618812561\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "epoch:  72  loss:  0.6931642293930054\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "epoch:  73  loss:  0.6931632161140442\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "epoch:  74  loss:  0.6931622624397278\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "epoch:  75  loss:  0.6931614875793457\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "epoch:  76  loss:  0.6931606531143188\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "epoch:  77  loss:  0.6931600570678711\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "epoch:  78  loss:  0.6931594014167786\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "epoch:  79  loss:  0.6931588649749756\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "epoch:  80  loss:  0.6931585669517517\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "epoch:  81  loss:  0.6931580305099487\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "epoch:  82  loss:  0.6931576132774353\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "epoch:  83  loss:  0.693157434463501\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "epoch:  84  loss:  0.6931570172309875\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "epoch:  85  loss:  0.6931567788124084\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "epoch:  86  loss:  0.6931564807891846\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "epoch:  87  loss:  0.6931563019752502\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "epoch:  88  loss:  0.6931561231613159\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "epoch:  89  loss:  0.6931559443473816\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "epoch:  90  loss:  0.6931557655334473\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "epoch:  91  loss:  0.6931556463241577\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "epoch:  92  loss:  0.6931555271148682\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "epoch:  93  loss:  0.6931554675102234\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "epoch:  94  loss:  0.6931554079055786\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "epoch:  95  loss:  0.6931552290916443\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "epoch:  96  loss:  0.6931551694869995\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "epoch:  97  loss:  0.69315505027771\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "epoch:  98  loss:  0.6931549906730652\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "epoch:  99  loss:  0.6931549310684204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BoWClassifier(num_labels=2, vocab_size=4575, num_hidden=7)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def train_loop(model, loss_func, optimizer, x, y):   \n",
    "    #compute prediction and loss\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred,y)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(model, loss_function, optimizer, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d744c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
