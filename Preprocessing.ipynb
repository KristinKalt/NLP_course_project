{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d55cea59ba497eaf6ff4f36eb2e123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_language_data(df):\n",
    "    def get_lang_df(df, language):\n",
    "        return df[df['language'] == language]\n",
    "    return get_lang_df(df_train, 'english').copy(), get_lang_df(df_train, 'finnish').copy(), get_lang_df(df_train, 'japanese').copy()\n",
    "\n",
    "df_train_EN, df_train_FI, df_train_JAP = split_language_data(df_train)\n",
    "df_val_EN, df_val_FI, df_val_JAP = split_language_data(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8786f5",
   "metadata": {},
   "source": [
    "###  (a) Tokenize question and document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    def make_col_answer(text):\n",
    "        return text['answer_text'][0]\n",
    "\n",
    "    def make_col_answer_start(text):\n",
    "        return text['answer_start'][0]\n",
    "\n",
    "    df['answer_text'] = df['annotations'].apply(make_col_answer)\n",
    "    df['answer_start'] = df['annotations'].apply(make_col_answer_start)\n",
    "    df['answerable'] = df['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize(df, col: str, language):\n",
    "    df[col+'_tokens'] = df[col].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(remove_stopwords, language=language)\n",
    "\n",
    "def helper_func_JAP(text):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(text)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df, col):\n",
    "    df[col+'_tokens'] = df[col].apply(helper_func_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4f21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize EN\n",
    "prepare_df(df_train_EN)\n",
    "tokenize(df_train_EN, 'question_text', 'english')\n",
    "tokenize(df_train_EN, 'document_plaintext', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7302c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize FI\n",
    "prepare_df(df_train_FI)\n",
    "tokenize(df_train_FI, 'question_text', 'finnish')\n",
    "tokenize(df_train_FI, 'document_plaintext', 'finnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize JAP\n",
    "prepare_df(df_train_JAP)\n",
    "tokenize_JAP(df_train_JAP, 'question_text')\n",
    "tokenize_JAP(df_train_JAP, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2b9ac",
   "metadata": {},
   "source": [
    "### (b) Compute most common first and last tokens in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['question_text_tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['question_text_tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "76ddcb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "born           342\n",
       "founded        204\n",
       "die            122\n",
       "have           104\n",
       "formed         100\n",
       "established     96\n",
       "air             82\n",
       "released        80\n",
       "live            76\n",
       "introduced      72\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "\n",
    "def make_bow_get_vocab_size(df):  \n",
    "\n",
    "    def get_question_vocab(df):\n",
    "        token_list_temp = df.question_text_tokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = get_question_vocab(df)\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_question'] = df['question_text_tokens_cleaned'].apply(transform_bow)\n",
    "    return len(df.iloc[0]['bow_question'][0])\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_text_tokens_cleaned']) & set(row['document_plaintext_tokens_cleaned'])))\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(vocab_size, num_labels)\n",
    "        #self.output = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771661",
   "metadata": {},
   "source": [
    "### Testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2045df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixe X test:  torch.Size([10, 7])\n",
      "epoch:  0  loss:  0.6307103633880615\n"
     ]
    }
   ],
   "source": [
    "#Test data\n",
    "x_test = torch.randn(10, 7)\n",
    "y_test = torch.tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "print(\"Sixe X test: \", x_test.size())\n",
    "\n",
    "test_model = BoWClassifier(num_labels=2, vocab_size=7, num_hidden=8)\n",
    "train_loop(test_model, loss_function, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad125947",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90263734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    temp_list = list(df['bow_question'].values)\n",
    "    temp_list_ii = [x[0] for x in temp_list]\n",
    "    y_list = list(df['answerable'].values)\n",
    "\n",
    "    X = torch.FloatTensor(temp_list_ii)\n",
    "    y = torch.tensor(y_list)\n",
    "\n",
    "    return train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51514bc",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "019d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, X, y):\n",
    "\n",
    "    def train_loop(model, X, y):   \n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        #compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad() #cleans the gradients\n",
    "        loss.backward() #computes the gradients\n",
    "        optimizer.step() #update the parameters\n",
    "            \n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loop(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d744c",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3957b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        y_pred_test = model(X)\n",
    "        y_pred = np.argmax(y_pred_test, axis=1)\n",
    "        y_pred_tensor = y_pred.type(torch.FloatTensor)\n",
    "        print(y_pred_tensor)\n",
    "        print(type(y_pred_tensor))\n",
    "        loss_test = loss_function(y_pred_tensor, y)\n",
    "        total_loss += float(loss_test)\n",
    "    print(\"Total loss on test data: \", total_loss)\n",
    "    print(classification_report(y_true=y, y_pred=y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755541b3",
   "metadata": {},
   "source": [
    "### Calling the methods for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476c2f4",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40cca60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1478])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_EN.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb3cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5867c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.,  ..., 0., 0., 1.])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#vocab_size_EN = make_bow_get_vocab_size(df_train_EN)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#X_train_EN, X_test_EN, y_train_EN, y_test_EN = prep_data(df_train_EN)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model_EN = BoWClassifier(num_labels=2, vocab_size=vocab_size_EN, num_hidden=vocab_size_EN)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#train_model(model_EN, X_train_EN, y_train_EN)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_EN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_EN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_EN\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [83], line 10\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_pred_tensor)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y_pred_tensor))\n\u001b[0;32m---> 10\u001b[0m     loss_test \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss_test)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal loss on test data: \u001b[39m\u001b[38;5;124m\"\u001b[39m, total_loss)\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1162'>1163</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1163'>1164</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1164'>1165</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1165'>1166</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/functional.py?line=3011'>3012</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/functional.py?line=3012'>3013</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/nn/functional.py?line=3013'>3014</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "#vocab_size_EN = make_bow_get_vocab_size(df_train_EN)\n",
    "#X_train_EN, X_test_EN, y_train_EN, y_test_EN = prep_data(df_train_EN)\n",
    "#model_EN = BoWClassifier(num_labels=2, vocab_size=vocab_size_EN, num_hidden=vocab_size_EN)\n",
    "#train_model(model_EN, X_train_EN, y_train_EN)\n",
    "test_model(model_EN, X_test_EN, y_test_EN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df4607",
   "metadata": {},
   "source": [
    "### FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2472ecfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m make_bow_get_vocab_size(df_train_FI)\n\u001b[1;32m      2\u001b[0m X_train_FI, X_test_FI, y_train_FI, y_test_FI \u001b[38;5;241m=\u001b[39m prep_data(df_train_FI)\n\u001b[0;32m----> 3\u001b[0m model_FI \u001b[38;5;241m=\u001b[39m \u001b[43mBoWClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_model(model_FI, X_train_FI, y_train_FI)\n\u001b[1;32m      5\u001b[0m test_model(model_FI, X_test_FI, y_test_FI)\n",
      "Cell \u001b[0;32mIn [63], line 9\u001b[0m, in \u001b[0;36mBoWClassifier.__init__\u001b[0;34m(self, num_labels, vocab_size, num_hidden)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(vocab_size, num_labels)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "vocab_size = make_bow_get_vocab_size(df_train_FI)\n",
    "X_train_FI, X_test_FI, y_train_FI, y_test_FI = prep_data(df_train_FI)\n",
    "model_FI = BoWClassifier(num_labels=2, vocab_size=vocab_size, num_hidden=7)\n",
    "train_model(model_FI, X_train_FI, y_train_FI)\n",
    "test_model(model_FI, X_test_FI, y_test_FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed644fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
