{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7abd7e5dea4b0f99cf482462ea09d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29868"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_df(df, language):\n",
    "    return df[df['language'] == language]\n",
    "\n",
    "df_train_FI = get_lang_df(df_train, 'finnish').copy()\n",
    "df_train_JAP = get_lang_df(df_train, 'japanese').copy()\n",
    "df_train_EN = get_lang_df(df_train, 'english').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize_EN(df, col: str):\n",
    "    df[col+'tokens'] = df[col].apply(word_tokenize)\n",
    "    df[col+'tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'tokens_cleaned'] = df[col+'tokens_cleaned'].apply(word_tokenize)\n",
    "    df[col+'tokens_cleaned'] = df[col+'tokens_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "def tokenize_FI(df):\n",
    "    df['tokens'] = df['question_text'].apply(word_tokenize, language='finnish')\n",
    "\n",
    "def helper_func_JAP(question):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(question)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df):\n",
    "    df['tokens'] = df['question_text'].apply(helper_func_JAP)\n",
    "\n",
    "\n",
    "tokenize_EN(df_train_EN, 'question_text')\n",
    "#tokenize_FI(df_train_FI)\n",
    "#tokenize_JAP(df_train_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3798'>3799</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mlast_token\u001b[38;5;241m.\u001b[39mvalue_counts()[:n]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#For English\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_res \u001b[38;5;241m=\u001b[39m \u001b[43mget_most_common_first_n_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_EN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m get_most_common_last_n_tokens(df_train_EN, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m, in \u001b[0;36mget_most_common_first_n_tokens\u001b[0;34m(df, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_most_common_first_n_tokens\u001b[39m(df, n):\n\u001b[0;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_token\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mfirst_token\u001b[38;5;241m.\u001b[39mvalue_counts()[:n]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3802'>3803</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3803'>3804</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3804'>3805</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3805'>3806</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3806'>3807</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3803'>3804</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3804'>3805</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3805'>3806</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3806'>3807</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'"
     ]
    }
   ],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]\n",
    "\n",
    "#For English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/z1y7t00n7bd8lm8r0r4sy2b40000gn/T/ipykernel_3860/2224523845.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  res_JAP.to_latex()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llrlr}\\n\\\\toprule\\n{} &        0 &    1 &   2 &     3 \\\\\\\\\\n\\\\midrule\\n0 &       日本 &  354 &   た &  2115 \\\\\\\\\\n1 &        『 &  306 &   か &  1305 \\\\\\\\\\n2 &       世界 &   94 &   何 &  1192 \\\\\\\\\\n3 &      ジョン &   58 &  いつ &   996 \\\\\\\\\\n4 &        第 &   56 &   は &   932 \\\\\\\\\\n5 &  アメリカ合衆国 &   54 &  どこ &   884 \\\\\\\\\\n6 &        「 &   50 &   誰 &   746 \\\\\\\\\\n7 &     アメリカ &   50 &  ある &   174 \\\\\\\\\\n8 &    ウィリアム &   44 &  だれ &    64 \\\\\\\\\\n9 &     ジョージ &   44 &  いる &    42 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)\n",
    "res_JAP.to_latex()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61501f79",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0173d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>question_texttokens</th>\n",
       "      <th>question_texttokens_cleaned</th>\n",
       "      <th>doc_tokens</th>\n",
       "      <th>word_count_doc</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [159], 'answer_text': ['1920s']}</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[quantum, field, theory, developed]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>31</td>\n",
       "      <td>1920s</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [610], 'answer_text': ['Sully...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[first, nobel, prize, winner, literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>188</td>\n",
       "      <td>Sully Prudhomme</td>\n",
       "      <td>610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [129], 'answer_text': ['disco...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[dialectical, method, used]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>113</td>\n",
       "      <td>discourse between two or more people holding d...</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [88], 'answer_text': ['Sejong...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[invented, hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>69</td>\n",
       "      <td>Sejong the Great</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [0], 'answer_text': ['Grassho...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[grasshoppers, eat]</td>\n",
       "      <td>[Grasshoppers, are, plant-eaters, ,, with, a, ...</td>\n",
       "      <td>125</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question_text  \\\n",
       "26            When was quantum field theory developed?   \n",
       "43   Who was the first Nobel prize winner for Liter...   \n",
       "112               When is the dialectical method used?   \n",
       "123                               Who invented Hangul?   \n",
       "125                          What do Grasshoppers eat?   \n",
       "\n",
       "                            document_title language  \\\n",
       "26                    Quantum field theory  english   \n",
       "43   List of Nobel laureates in Literature  english   \n",
       "112                              Dialectic  english   \n",
       "123                       Origin of Hangul  english   \n",
       "125                            Grasshopper  english   \n",
       "\n",
       "                                           annotations  \\\n",
       "26   {'answer_start': [159], 'answer_text': ['1920s']}   \n",
       "43   {'answer_start': [610], 'answer_text': ['Sully...   \n",
       "112  {'answer_start': [129], 'answer_text': ['disco...   \n",
       "123  {'answer_start': [88], 'answer_text': ['Sejong...   \n",
       "125  {'answer_start': [0], 'answer_text': ['Grassho...   \n",
       "\n",
       "                                    document_plaintext  \\\n",
       "26   Quantum field theory naturally began with the ...   \n",
       "43   The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "112  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "123  Hangul was personally created and promulgated ...   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                          document_url  \\\n",
       "26   https://en.wikipedia.org/wiki/Quantum%20field%...   \n",
       "43   https://en.wikipedia.org/wiki/List%20of%20Nobe...   \n",
       "112            https://en.wikipedia.org/wiki/Dialectic   \n",
       "123  https://en.wikipedia.org/wiki/Origin%20of%20Ha...   \n",
       "125          https://en.wikipedia.org/wiki/Grasshopper   \n",
       "\n",
       "                                   question_texttokens  \\\n",
       "26   [When, was, quantum, field, theory, developed, ?]   \n",
       "43   [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "112      [When, is, the, dialectical, method, used, ?]   \n",
       "123                         [Who, invented, Hangul, ?]   \n",
       "125                   [What, do, Grasshoppers, eat, ?]   \n",
       "\n",
       "                   question_texttokens_cleaned  \\\n",
       "26         [quantum, field, theory, developed]   \n",
       "43   [first, nobel, prize, winner, literature]   \n",
       "112                [dialectical, method, used]   \n",
       "123                         [invented, hangul]   \n",
       "125                        [grasshoppers, eat]   \n",
       "\n",
       "                                            doc_tokens  word_count_doc  \\\n",
       "26   [Quantum, field, theory, naturally, began, wit...              31   \n",
       "43   [The, Nobel, Prize, in, Literature, (, Swedish...             188   \n",
       "112  [Dialectic, or, dialectics, (, Greek, :, διαλε...             113   \n",
       "123  [Hangul, was, personally, created, and, promul...              69   \n",
       "125  [Grasshoppers, are, plant-eaters, ,, with, a, ...             125   \n",
       "\n",
       "                                           answer_text  answer_start  \\\n",
       "26                                               1920s           159   \n",
       "43                                     Sully Prudhomme           610   \n",
       "112  discourse between two or more people holding d...           129   \n",
       "123                                   Sejong the Great            88   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...             0   \n",
       "\n",
       "     answerable  \n",
       "26            1  \n",
       "43            1  \n",
       "112           1  \n",
       "123           1  \n",
       "125           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_col_answer(text):\n",
    "    return text['answer_text'][0]\n",
    "\n",
    "def make_col_answer_start(text):\n",
    "    return text['answer_start'][0]\n",
    "\n",
    "df_train_EN['answer_text'] = df_train_EN['annotations'].apply(make_col_answer)\n",
    "df_train_EN['answer_start'] = df_train_EN['annotations'].apply(make_col_answer_start)\n",
    "df_train_EN['answerable'] = df_train_EN['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "df_train_EN.head()\n",
    "\n",
    "tokenize_EN(df_train_EN, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3243f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train_EN[df_train_EN['answerable'] == 0].iloc[82]['document_plaintexttokens_cleaned'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "def get_bow(df):\n",
    "    def get_question_vocab():\n",
    "        token_list_temp = df_train_EN.question_texttokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "    vocab = get_question_vocab()\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_question'] = df['question_texttokens_cleaned'].apply(transform_bow)\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_texttokens_cleaned']) & set(row['document_plaintexttokens_cleaned'])))\n",
    "    #df['overlap_doc_question'] = df['question_texttokens_cleaned'].apply(set) & df['document_plaintexttokens_cleaned'].apply(set)\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#count_words_in_doc(df_train_EN)\n",
    "get_bow(df_train_EN)\n",
    "#get_overlap(df_train_EN)\n",
    "df_train_EN.iloc[0]['bow_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "618ecfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['diff', 'is', 'test', 'this'], dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = [\"this\", \"is\", \"test\"]\n",
    "doc_2 = [\"this\", \"test\"]\n",
    "doc_3 = [\"this\", \"diff\"]\n",
    "\n",
    "vocab = doc_1 + doc_2 + doc_3\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(vocab)\n",
    "\n",
    "vectorizer.transform(doc_1).toarray()\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d2d441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(doc_1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden=2):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_hidden)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(num_hidden, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c1360b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7389"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "90263734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/z1y7t00n7bd8lm8r0r4sy2b40000gn/T/ipykernel_3960/3144699533.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  X = torch.tensor(temp_list)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m      5\u001b[0m temp_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_train_EN[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbow_question\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(df_train_EN[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswerable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 5)"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "train_data = df_train_EN[['bow_question', 'answerable']]\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "temp_list = list(df_train_EN['bow_question'].values)\n",
    "\n",
    "X = torch.tensor(temp_list)\n",
    "y = torch.tensor(df_train_EN['answerable'].values)\n",
    "\n",
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "35400aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a10e738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3393, -0.3436, -0.2345,  0.2599, -0.1353,  0.2573, -0.2947, -0.0647],\n",
      "        [ 0.1685, -0.2212, -0.2073,  0.1736, -0.1699,  0.0307,  0.0142,  0.2243]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.1082], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2744,  0.5941],\n",
      "        [ 0.3179,  0.4292]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3868, -0.1938], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(num_labels=2, vocab_size=len(vocab))\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "019d988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3798'>3799</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [122], line 5\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_func, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(dataloader, model, loss_func, optimizer):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m#compute prediction and loss\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m      8\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_func(pred,y)\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=677'>678</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=678'>679</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=679'>680</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=680'>681</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=681'>682</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=682'>683</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=683'>684</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=684'>685</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=718'>719</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=719'>720</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=720'>721</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=721'>722</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=722'>723</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3802'>3803</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3803'>3804</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3804'>3805</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3805'>3806</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3806'>3807</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3799'>3800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3803'>3804</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3804'>3805</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3805'>3806</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/dpr577/Workspace/NLP_course/NLP_course_project/nlp-venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3806'>3807</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def train_loop(dataloader, model, loss_func, optimizer):\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        #compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_func(pred,y)\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_function, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d744c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
