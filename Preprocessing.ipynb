{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e4c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc98e5",
   "metadata": {},
   "source": [
    "## Load data set into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/dpr577/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d55cea59ba497eaf6ff4f36eb2e123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_val = pd.DataFrame(validation_set)\n",
    "df_val = df_val[df_val.language.isin(['finnish', 'english', 'japanese'])]\n",
    "\n",
    "df_train = pd.DataFrame(train_set)\n",
    "df_train = df_train[df_train.language.isin(['finnish', 'english', 'japanese'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1a38c",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_language_data(df):\n",
    "    def get_lang_df(df, language):\n",
    "        return df[df['language'] == language]\n",
    "    return get_lang_df(df_train, 'english').copy(), get_lang_df(df_train, 'finnish').copy(), get_lang_df(df_train, 'japanese').copy()\n",
    "\n",
    "df_train_EN, df_train_FI, df_train_JAP = split_language_data(df_train)\n",
    "df_val_EN, df_val_FI, df_val_JAP = split_language_data(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8786f5",
   "metadata": {},
   "source": [
    "###  (a) Tokenize question and document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    def make_col_answer(text):\n",
    "        return text['answer_text'][0]\n",
    "\n",
    "    def make_col_answer_start(text):\n",
    "        return text['answer_start'][0]\n",
    "\n",
    "    df['answer_text'] = df['annotations'].apply(make_col_answer)\n",
    "    df['answer_start'] = df['annotations'].apply(make_col_answer_start)\n",
    "    df['answerable'] = df['answer_start'].apply(lambda x : 0 if x == -1 else 1)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\".join([char.lower() for char in text if char not in string.punctuation]) \n",
    "\n",
    "def remove_stopwords(tokens, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "\n",
    "def tokenize(df, col: str, language):\n",
    "    df[col+'_tokens'] = df[col].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col].apply(clean_text)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(word_tokenize, language=language)\n",
    "    df[col+'_tokens_cleaned'] = df[col+'_tokens_cleaned'].apply(remove_stopwords, language=language)\n",
    "\n",
    "def helper_func_JAP(text):\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    res_list = tokenizer_obj.tokenize(text)\n",
    "    return [x.surface() for x in res_list]\n",
    "\n",
    "def tokenize_JAP(df, col):\n",
    "    df[col+'_tokens'] = df[col].apply(helper_func_JAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4f21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize EN\n",
    "prepare_df(df_train_EN)\n",
    "tokenize(df_train_EN, 'question_text', 'english')\n",
    "tokenize(df_train_EN, 'document_plaintext', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7302c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize FI\n",
    "prepare_df(df_train_FI)\n",
    "tokenize(df_train_FI, 'question_text', 'finnish')\n",
    "tokenize(df_train_FI, 'document_plaintext', 'finnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize JAP\n",
    "prepare_df(df_train_JAP)\n",
    "tokenize_JAP(df_train_JAP, 'question_text')\n",
    "tokenize_JAP(df_train_JAP, 'document_plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421b05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'ダン', '”', ' ', 'ダニエル', '・', 'ジャドソン', '・', 'キャラハン', 'の', '出身', 'は', 'どこ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ\"\n",
    "helper_func_JAP(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2b9ac",
   "metadata": {},
   "source": [
    "### (b) Compute most common first and last tokens in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1473ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_first_n_tokens(df, n):\n",
    "    df['first_token'] = df['question_text_tokens'].apply(lambda x: x[0])\n",
    "    return df.first_token.value_counts()[:n]\n",
    "\n",
    "def get_most_common_last_n_tokens(df, n):\n",
    "    df['last_token'] = df['question_text_tokens'].apply(lambda x: x[-1] if x[-1].isalpha() else x[-2])\n",
    "    return df.last_token.value_counts()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "76ddcb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "born           342\n",
       "founded        204\n",
       "die            122\n",
       "have           104\n",
       "formed         100\n",
       "established     96\n",
       "air             82\n",
       "released        80\n",
       "live            76\n",
       "introduced      72\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English\n",
    "df_res = get_most_common_first_n_tokens(df_train_EN, 10)\n",
    "get_most_common_last_n_tokens(df_train_EN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c55c15fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syntyi          1072\n",
       "on               723\n",
       "kuoli            720\n",
       "tarkoittaa       488\n",
       "perustettu       476\n",
       "syntynyt         398\n",
       "oli              382\n",
       "perustettiin     351\n",
       "sijaitsee        258\n",
       "pinta-ala        214\n",
       "Name: last_token, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finnish\n",
    "get_most_common_first_n_tokens(df_train_FI, 10)\n",
    "get_most_common_last_n_tokens(df_train_FI, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1ef7f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Japanese\n",
    "res_JAP_first = get_most_common_first_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP_last = get_most_common_last_n_tokens(df_train_JAP, 10).reset_index()\n",
    "res_JAP = pd.concat([res_JAP_first, res_JAP_last], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edbbb3",
   "metadata": {},
   "source": [
    "## 1.2 Binary Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "124b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f88ba6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9f17cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_doc(df):\n",
    "    df['doc_tokens'] = df['document_plaintext'].apply(word_tokenize)\n",
    "    df[\"word_count_doc\"] = df['doc_tokens'].str.len()\n",
    "\n",
    "\n",
    "def make_bow_get_vocab_size(df):  \n",
    "\n",
    "    def get_question_vocab(df):\n",
    "        token_list_temp = df.question_text_tokens_cleaned.to_list()\n",
    "        return  [item for sublist in token_list_temp for item in sublist]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = get_question_vocab(df)\n",
    "    vectorizer.fit(vocab)\n",
    "\n",
    "    def transform_bow(cell):\n",
    "        text = [\" \".join(cell)]\n",
    "        res = vectorizer.transform(text)\n",
    "        return res.toarray()\n",
    "    \n",
    "    df['bow_question'] = df['question_text_tokens_cleaned'].apply(transform_bow)\n",
    "    return len(df.iloc[0]['bow_question'][0])\n",
    "\n",
    "def get_overlap(df):\n",
    "    def calculate_overlap(row):\n",
    "        return len(list(set(row['question_text_tokens_cleaned']) & set(row['document_plaintext_tokens_cleaned'])))\n",
    "    df['overlap_doc_question'] = df.apply(calculate_overlap, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db5135",
   "metadata": {},
   "source": [
    "### Building the Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f422fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, num_hidden):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.final = nn.Linear(vocab_size, num_labels)\n",
    "        #self.output = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, bow_vector):\n",
    "        return self.final(self.nonlinear(self.linear(bow_vector)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771661",
   "metadata": {},
   "source": [
    "### Testing the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2045df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixe X test:  torch.Size([10, 7])\n",
      "epoch:  0  loss:  0.6307103633880615\n"
     ]
    }
   ],
   "source": [
    "#Test data\n",
    "x_test = torch.randn(10, 7)\n",
    "y_test = torch.tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "print(\"Sixe X test: \", x_test.size())\n",
    "\n",
    "test_model = BoWClassifier(num_labels=2, vocab_size=7, num_hidden=8)\n",
    "train_loop(test_model, loss_function, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad125947",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "90263734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    temp_list = list(df['bow_question'].values)\n",
    "    temp_list_ii = [x[0] for x in temp_list]\n",
    "    y_list = list(df['answerable'].values)\n",
    "\n",
    "    X = torch.FloatTensor(temp_list_ii)\n",
    "    y = torch.tensor(y_list)\n",
    "\n",
    "    return train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51514bc",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, X, y):\n",
    "\n",
    "    def train_loop(model, X, y):   \n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        #compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad() #cleans the gradients\n",
    "        loss.backward() #computes the gradients\n",
    "        optimizer.step() #update the parameters\n",
    "            \n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loop(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d744c",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3957b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        y_pred_test = model(X)\n",
    "        y_pred_test = y_pred_test.detach().cpu().numpy()\n",
    "        y_pred = np.argmax(y_pred_test, axis=1)\n",
    "        y_pred_tensor = torch.FloatTensor(y_pred)\n",
    "\n",
    "        y_true = y.type(torch.FloatTensor)\n",
    "        \n",
    "        loss_test = loss_function(y_pred_tensor, y_true)\n",
    "        total_loss += float(loss_test)\n",
    "    print(\"Total loss on test data: \", total_loss)\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755541b3",
   "metadata": {},
   "source": [
    "### Calling the methods for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476c2f4",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5867c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6931540966033936\n",
      "epoch:  1  loss:  0.6931519508361816\n",
      "epoch:  2  loss:  0.693149983882904\n",
      "epoch:  3  loss:  0.6931478381156921\n",
      "epoch:  4  loss:  0.693145751953125\n",
      "epoch:  5  loss:  0.6931436061859131\n",
      "epoch:  6  loss:  0.693141520023346\n",
      "epoch:  7  loss:  0.693139374256134\n",
      "epoch:  8  loss:  0.6931373476982117\n",
      "epoch:  9  loss:  0.6931352019309998\n",
      "epoch:  10  loss:  0.6931331157684326\n",
      "epoch:  11  loss:  0.6931310296058655\n",
      "epoch:  12  loss:  0.6931288838386536\n",
      "epoch:  13  loss:  0.6931267380714417\n",
      "epoch:  14  loss:  0.6931247115135193\n",
      "epoch:  15  loss:  0.6931226253509521\n",
      "epoch:  16  loss:  0.6931204795837402\n",
      "epoch:  17  loss:  0.6931183934211731\n",
      "epoch:  18  loss:  0.6931162476539612\n",
      "epoch:  19  loss:  0.6931142210960388\n",
      "epoch:  20  loss:  0.6931121349334717\n",
      "epoch:  21  loss:  0.6931100487709045\n",
      "epoch:  22  loss:  0.6931079030036926\n",
      "epoch:  23  loss:  0.6931057572364807\n",
      "epoch:  24  loss:  0.6931037902832031\n",
      "epoch:  25  loss:  0.6931015849113464\n",
      "epoch:  26  loss:  0.6930994987487793\n",
      "epoch:  27  loss:  0.6930975317955017\n",
      "epoch:  28  loss:  0.6930953860282898\n",
      "epoch:  29  loss:  0.6930932402610779\n",
      "epoch:  30  loss:  0.6930911540985107\n",
      "epoch:  31  loss:  0.6930890679359436\n",
      "epoch:  32  loss:  0.6930869221687317\n",
      "epoch:  33  loss:  0.6930848956108093\n",
      "epoch:  34  loss:  0.6930827498435974\n",
      "epoch:  35  loss:  0.6930806040763855\n",
      "epoch:  36  loss:  0.6930785179138184\n",
      "epoch:  37  loss:  0.6930765509605408\n",
      "epoch:  38  loss:  0.6930744051933289\n",
      "epoch:  39  loss:  0.6930722594261169\n",
      "epoch:  40  loss:  0.6930702328681946\n",
      "epoch:  41  loss:  0.6930680871009827\n",
      "epoch:  42  loss:  0.6930659413337708\n",
      "epoch:  43  loss:  0.6930639743804932\n",
      "epoch:  44  loss:  0.693061888217926\n",
      "epoch:  45  loss:  0.6930597424507141\n",
      "epoch:  46  loss:  0.6930577158927917\n",
      "epoch:  47  loss:  0.6930555701255798\n",
      "epoch:  48  loss:  0.6930534839630127\n",
      "epoch:  49  loss:  0.6930513978004456\n",
      "epoch:  50  loss:  0.6930492520332336\n",
      "epoch:  51  loss:  0.6930472254753113\n",
      "epoch:  52  loss:  0.6930451393127441\n",
      "epoch:  53  loss:  0.693043053150177\n",
      "epoch:  54  loss:  0.6930409073829651\n",
      "epoch:  55  loss:  0.6930388808250427\n",
      "epoch:  56  loss:  0.6930367946624756\n",
      "epoch:  57  loss:  0.6930347084999084\n",
      "epoch:  58  loss:  0.6930326819419861\n",
      "epoch:  59  loss:  0.6930305361747742\n",
      "epoch:  60  loss:  0.693028450012207\n",
      "epoch:  61  loss:  0.6930263638496399\n",
      "epoch:  62  loss:  0.693024218082428\n",
      "epoch:  63  loss:  0.6930221915245056\n",
      "epoch:  64  loss:  0.6930201053619385\n",
      "epoch:  65  loss:  0.6930179595947266\n",
      "epoch:  66  loss:  0.6930158734321594\n",
      "epoch:  67  loss:  0.6930139064788818\n",
      "epoch:  68  loss:  0.6930117607116699\n",
      "epoch:  69  loss:  0.6930097341537476\n",
      "epoch:  70  loss:  0.6930076479911804\n",
      "epoch:  71  loss:  0.6930055618286133\n",
      "epoch:  72  loss:  0.6930035352706909\n",
      "epoch:  73  loss:  0.693001389503479\n",
      "epoch:  74  loss:  0.6929993033409119\n",
      "epoch:  75  loss:  0.6929972171783447\n",
      "epoch:  76  loss:  0.6929950714111328\n",
      "epoch:  77  loss:  0.6929931044578552\n",
      "epoch:  78  loss:  0.6929909586906433\n",
      "epoch:  79  loss:  0.6929890513420105\n",
      "epoch:  80  loss:  0.6929868459701538\n",
      "epoch:  81  loss:  0.6929847598075867\n",
      "epoch:  82  loss:  0.6929826736450195\n",
      "epoch:  83  loss:  0.6929806470870972\n",
      "epoch:  84  loss:  0.6929785013198853\n",
      "epoch:  85  loss:  0.6929764151573181\n",
      "epoch:  86  loss:  0.6929744482040405\n",
      "epoch:  87  loss:  0.6929721832275391\n",
      "epoch:  88  loss:  0.6929701566696167\n",
      "epoch:  89  loss:  0.6929681301116943\n",
      "epoch:  90  loss:  0.6929659247398376\n",
      "epoch:  91  loss:  0.6929639577865601\n",
      "epoch:  92  loss:  0.6929618120193481\n",
      "epoch:  93  loss:  0.692959725856781\n",
      "epoch:  94  loss:  0.6929576396942139\n",
      "epoch:  95  loss:  0.6929556727409363\n",
      "epoch:  96  loss:  0.6929535269737244\n",
      "epoch:  97  loss:  0.6929514408111572\n",
      "epoch:  98  loss:  0.6929494142532349\n",
      "epoch:  99  loss:  0.6929473280906677\n",
      "Total loss on test data:  5277.53125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.03      0.05       758\n",
      "         1.0       0.48      0.94      0.63       720\n",
      "\n",
      "    accuracy                           0.47      1478\n",
      "   macro avg       0.40      0.48      0.34      1478\n",
      "weighted avg       0.40      0.47      0.33      1478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size_EN = make_bow_get_vocab_size(df_train_EN)\n",
    "X_train_EN, X_test_EN, y_train_EN, y_test_EN = prep_data(df_train_EN)\n",
    "model_EN = BoWClassifier(num_labels=2, vocab_size=vocab_size_EN, num_hidden=vocab_size_EN)\n",
    "train_model(model_EN, X_train_EN, y_train_EN)\n",
    "test_model(model_EN, X_test_EN, y_test_EN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df4607",
   "metadata": {},
   "source": [
    "### FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2472ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6931490302085876\n",
      "epoch:  1  loss:  0.6931475400924683\n",
      "epoch:  2  loss:  0.6931461691856384\n",
      "epoch:  3  loss:  0.6931449174880981\n",
      "epoch:  4  loss:  0.6931435465812683\n",
      "epoch:  5  loss:  0.693142294883728\n",
      "epoch:  6  loss:  0.6931410431861877\n",
      "epoch:  7  loss:  0.6931398510932922\n",
      "epoch:  8  loss:  0.6931387186050415\n",
      "epoch:  9  loss:  0.693137526512146\n",
      "epoch:  10  loss:  0.6931365132331848\n",
      "epoch:  11  loss:  0.6931353211402893\n",
      "epoch:  12  loss:  0.6931342482566833\n",
      "epoch:  13  loss:  0.6931332349777222\n",
      "epoch:  14  loss:  0.6931321024894714\n",
      "epoch:  15  loss:  0.693131148815155\n",
      "epoch:  16  loss:  0.6931300759315491\n",
      "epoch:  17  loss:  0.6931290626525879\n",
      "epoch:  18  loss:  0.6931279897689819\n",
      "epoch:  19  loss:  0.6931269764900208\n",
      "epoch:  20  loss:  0.6931260824203491\n",
      "epoch:  21  loss:  0.6931250095367432\n",
      "epoch:  22  loss:  0.693123996257782\n",
      "epoch:  23  loss:  0.6931230425834656\n",
      "epoch:  24  loss:  0.6931220293045044\n",
      "epoch:  25  loss:  0.6931210160255432\n",
      "epoch:  26  loss:  0.6931200623512268\n",
      "epoch:  27  loss:  0.6931190490722656\n",
      "epoch:  28  loss:  0.6931179761886597\n",
      "epoch:  29  loss:  0.6931169629096985\n",
      "epoch:  30  loss:  0.6931160092353821\n",
      "epoch:  31  loss:  0.6931149959564209\n",
      "epoch:  32  loss:  0.6931140422821045\n",
      "epoch:  33  loss:  0.6931130290031433\n",
      "epoch:  34  loss:  0.6931120157241821\n",
      "epoch:  35  loss:  0.693111002445221\n",
      "epoch:  36  loss:  0.6931100487709045\n",
      "epoch:  37  loss:  0.6931091547012329\n",
      "epoch:  38  loss:  0.6931081414222717\n",
      "epoch:  39  loss:  0.6931071281433105\n",
      "epoch:  40  loss:  0.6931061744689941\n",
      "epoch:  41  loss:  0.6931052207946777\n",
      "epoch:  42  loss:  0.6931042075157166\n",
      "epoch:  43  loss:  0.6931031942367554\n",
      "epoch:  44  loss:  0.693102240562439\n",
      "epoch:  45  loss:  0.6931012868881226\n",
      "epoch:  46  loss:  0.6931003332138062\n",
      "epoch:  47  loss:  0.693099319934845\n",
      "epoch:  48  loss:  0.6930983662605286\n",
      "epoch:  49  loss:  0.6930973529815674\n",
      "epoch:  50  loss:  0.6930962800979614\n",
      "epoch:  51  loss:  0.6930953860282898\n",
      "epoch:  52  loss:  0.6930943727493286\n",
      "epoch:  53  loss:  0.6930934190750122\n",
      "epoch:  54  loss:  0.6930925250053406\n"
     ]
    }
   ],
   "source": [
    "vocab_size_FI = make_bow_get_vocab_size(df_train_FI)\n",
    "X_train_FI, X_test_FI, y_train_FI, y_test_FI = prep_data(df_train_FI)\n",
    "model_FI = BoWClassifier(num_labels=2, vocab_size=vocab_size_FI, num_hidden=vocab_size_FI)\n",
    "train_model(model_FI, X_train_FI, y_train_FI)\n",
    "test_model(model_FI, X_test_FI, y_test_FI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed644fd",
   "metadata": {},
   "source": [
    "### JAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_JAP = make_bow_get_vocab_size(df_train_JAP)\n",
    "X_train_JAP, X_test_JAP, y_train_JAP, y_test_JAP = prep_data(df_train_JAP)\n",
    "model_JAP = BoWClassifier(num_labels=2, vocab_size=vocab_size_JAP, num_hidden=vocab_size_JAP)\n",
    "train_model(model_JAP, X_train_JAP, y_train_JAP)\n",
    "test_model(model_JAP, X_test_JAP, y_test_JAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9c75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
